I understand your request for 40-50 unique questions based on the "Data Analytics - Cheat Sheet.md" source. I will provide them in the exact 1 to 1 format you specified previously, without any deviations or additional styling like bolding, as confirmed in our conversation history.

Here is your quiz:

***

Quiz:

1. What is the approximate amount of data created every day?
    a. 2.5 billion bytes
    b. 2.5 million TB
    c. 2.5 quintillion bytes
    d. 2.5 petabytes

2. According to the sources, what percentage of the world's data has been generated in the last two years?
    a. 50%
    b. 75%
    c. 90%
    d. 100%

3. What is the foundation of data analytics, often targeted towards business problems or objectives?
    a. Data processing tools
    b. Data driven questions
    c. Statistical models
    d. Business intelligence dashboards

4. Which of the following is considered one of the three types of questions in data analytics?
    a. How much revenue did we generate?
    b. Do we have a problem to solve?
    c. When did this problem occur?
    d. Who is responsible for the data?

5. Which common reason for solving a problem directly impacts profitability due to fewer sales?
    a. Poor User Experience
    b. Increased Customer Acquisition Cost (CAC)
    c. Direct Revenue Loss
    d. Negative Brand Perception

6. What is the definition of Data Analytics?
    a. The process of collecting and storing data
    b. The process of examining, transforming and interpreting data to discover meaningful patterns and trends
    c. The process of creating charts and graphs for presentations
    d. The process of inputting data into a database

7. Which type of data analytics answers the question "What happened"?
    a. Diagnostic Analytics
    b. Predictive Analytics
    c. Prescriptive Analytics
    d. Descriptive Analytics

8. Which type of data analytics explores "Why that happened" by identifying causes and patterns?
    a. Descriptive Analytics
    b. Diagnostic Analytics
    c. Predictive Analytics
    d. Prescriptive Analytics

9. Which type of data analytics uses historical data to make predictions about future outcomes?
    a. Descriptive Analytics
    b. Diagnostic Analytics
    c. Predictive Analytics
    d. Prescriptive Analytics

10. Which type of data analytics recommends actions to achieve a desired outcome?
    a. Descriptive Analytics
    b. Diagnostic Analytics
    c. Predictive Analytics
    d. Prescriptive Analytics

11. What is the definition of data collection?
    a. Analyzing data patterns and relationships
    b. Preparing data by removing errors
    c. Gathering relevant data from different sources
    d. Applying statistical models to make predictions

12. Which of the following is a common source for data collection?
    a. Newspapers
    b. Surveys
    c. Encyclopedias
    d. Phone books

13. What is a key goal of data collection?
    a. To generate reports
    b. To create a comprehensive dataset to fulfill an objective
    c. To store data securely
    d. To validate statistical models

14. What is the definition of data cleaning?
    a. Gathering data from multiple sources
    b. Analyzing data patterns for insights
    c. Preparing the data by removing errors, duplicates, and inconsistencies
    d. Presenting data visually in dashboards

15. Which of the following is a keyword associated with data cleaning?
    a. Identify data sources
    b. Handle missing values
    c. Make predictions
    d. Summarize insights

16. What is the goal of data cleaning?
    a. To get a dataset ready for analysis with minimal biases or errors
    b. To identify new data sources
    c. To apply machine learning algorithms
    d. To communicate findings to stakeholders

17. What is the first step in the summary of data cleaning steps?
    a. Removing Duplicates
    b. Correcting Data Types
    c. Handling Missing Values
    d. Removing Invalid Data

18. What action is suggested if "Quantity" is negative during data cleaning?
    a. Standardize the data
    b. Remove the rows or adjust analysis to account for returned items
    c. Correct the data type
    d. Fill with default values

19. In the example provided for data cleaning, what change was made to "Invalid Date" in the "Purchase Date" field?
    a. Converted to a default date
    b. Marked as (missing)
    c. Converted to a numeric value
    d. Removed the entire row

20. In the example provided for data cleaning, what was done with blank "Discount" fields?
    a. Set to 100%
    b. Removed the rows
    c. Set to 0%
    d. Marked as (missing)

21. What is data exploration often referred to as?
    a. The final step of data analysis
    b. The first step of analyzing data
    c. The process of data storage
    d. The process of database management

22. Data exploration and data cleaning are often an iterative process. What does this mean?
    a. They are performed only once at the beginning.
    b. They must be done in strict sequential order.
    c. While data exploration, more cleaning might be needed.
    d. They are unrelated processes.

23. Which of the following is an "unofficial step" of data exploration related to "Quality Assessment"?
    a. Note data sources and any merges
    b. Check for missing/null values and decide how to handle them
    c. List variables (columns) and types
    d. Summarize key insights

24. What type of analysis involves calculating the average, minimum, and maximum sales for each month?
    a. Bivariate Analysis
    b. Multivariate Analysis
    c. Descriptive Statistics (Univariate Analysis)
    d. Inferential Statistics

25. Which data visualization is used to show which product categories generate the most revenue?
    a. Histogram of Order Values
    b. Box Plot of Order Values
    c. Bar Chart of Sales by Product Category
    d. Line Chart of Monthly Sales Trends

26. Which data visualization displays the distribution of individual order totals, helping to see skew or common purchase sizes?
    a. Bar Chart of Sales by Product Category
    b. Histogram of Order Values
    c. Scatter Plot of Sales vs. Advertising Spend
    d. Heatmap of Sales by Region

27. Which data visualization is best for assessing correlation between total sales and ad spend?
    a. Line Chart of Monthly Sales Trends
    b. Heatmap of Sales by Region
    c. Contingency Table of Region vs. Product Category
    d. Scatter Plot of Sales vs. Advertising Spend

28. What is the definition of Data Modeling?
    a. The process of applying statistical models to make predictions
    b. The process of creating charts and dashboards
    c. The process of creating the "blueprint" for how you’ll store and organize your data
    d. The process of gathering data from various sources

29. Data models are built around an organization’s what?
    a. Existing database systems
    b. Specific business needs
    c. Available data analysis tools
    d. Technical team's preferences

30. Which of the following is one of the key parts of data modeling in simple terms?
    a. Algorithms
    b. Reports
    c. Entities
    d. Queries

31. What is the first step in the provided steps of Data Modeling?
    a. Detail Attributes & Keys (Logical Model)
    b. Translate to Physical Model
    c. Gather Business Requirements
    d. Identify Entities & Relationships (Conceptual Model)

32. Which type of data model sketches high-level entity-relationship diagrams without detailed attributes?
    a. Physical Model
    b. Logical Model
    c. Conceptual Model
    d. Relational Model

33. Which step in data modeling involves defining all entities, attributes, and relationships, including primary and foreign keys, without specifying physical implementation?
    a. Gather Business Requirements
    b. Define Scope & Boundaries
    c. Identify Entities & Relationships (Conceptual Model)
    d. Detail Attributes & Keys (Logical Model)

34. What is the definition of Data Interpretation?
    a. The stage where you collect raw data for analysis
    b. The stage where you cleanse data by removing errors
    c. The stage where you turn raw analysis results into clear, concise insights
    d. The stage where you design database schemas

35. Which of the following is a key element of interpretation related to "Communicate Findings"?
    a. Identify Limitations
    b. Contextualize Results
    c. Summarize Insights
    d. Propose Next Steps

36. When interpreting findings, why is it important to "Identify Limitations"?
    a. To explain how to collect more data
    b. To call out data or analysis caveats
    c. To justify further analysis projects
    d. To validate hypotheses with more rigor

37. What is the first step in the "2 Steps Process" alternative take on data analytics?
    a. Data Exploration
    b. Data Preparation
    c. Data Modeling
    d. Data Interpretation

38. What is the definition of Exploratory Data Analysis (EDA)?
    a. The process of collecting and storing data for future use.
    b. The process of creating predictive models from raw data.
    c. The process of using statistical summaries and visualizations to systematically uncover patterns, anomalies, and relationships in a dataset.
    d. The process of formal hypothesis testing to confirm specific theories.

39. What is the initial step in doing EDA according to the sources?
    a. Import & Inspect the Data
    b. Handle Missing Data
    c. Define the Problem & Understand Your Data
    d. Transform Your Data for Analysis

40. What is an action involved in "Step 2: Import & Inspect the Data" during EDA?
    a. Compute basic summary statistics
    b. Verify Data Types
    c. Apply math transforms like logarithms
    d. Create new features from existing ones

41. In EDA, what is the strategy that preserves dataset size but must be done thoughtfully?
    a. Removal of missing data
    b. Imputation of missing data
    c. Ignoring missing data
    d. Deleting the dataset

42. Which simple imputation method is suggested for numerical fields during EDA?
    a. Regression
    b. K-nearest neighbors
    c. Mean or median imputation
    d. Decision-tree methods

43. What is involved in "Step 4: Exploring Your Data’s Key Characteristics" during EDA?
    a. Creating new predictors from existing ones
    b. Checking Distributions & Spot Oddities
    c. Converting categories into numbers
    d. Applying one-hot encoding

44. What does Min-Max Scaling do?
    a. Centers data at zero with unit variance.
    b. Assigns each category a unique integer.
    c. Rescales numeric features into a fixed range (usually 0 to 1).
    d. Applies a logarithm to data.

45. What transformation method turns a categorical variable with *k* distinct values into *k* binary (0/1) columns?
    a. Label Encoding
    b. Standardization (Z-Score)
    c. One-Hot Encoding
    d. Log Transformation

46. What type of transformation is useful for reducing right-skew and stabilizing variance in data?
    a. Min-Max Scaling
    b. Label Encoding
    c. Log Transformation
    d. One-Hot Encoding

47. When visualizing relationships in data, what type of chart is recommended for categorical variables to compare category counts?
    a. Scatter plots
    b. Histograms
    c. Bar charts or pie charts
    d. Heatmap-style correlation matrices

48. What type of visualization is used to examine each numerical feature’s distribution, central tendency, variability, and outliers?
    a. Frequency tables
    b. Bar charts
    c. Histograms, box plots, violin plots or density curves
    d. Contingency tables

49. What is the final step in the EDA process?
    a. Handle Missing Data
    b. Transform Your Data for Analysis
    c. Communicate Your Findings
    d. Explore Your Data’s Key Characteristics

50. When communicating findings, what should be done to make complex insights more digestible?
    a. Provide raw data tables
    b. Use charts and tables to illustrate key results
    c. Avoid any visual aids
    d. Only present statistical summaries

***

Answers:

1. c. 2.5 quintillion bytes
    - 2,500,000,000,000,000,000 Bytes (~2.5 quintillion bytes) of data are created every day.

2. c. 90%
    - 90% of the data in the world has been generated in the last two years.

3. b. Data driven questions
    - Data driven questions are the foundation of data analytics and are usually targeted towards problems or objectives a business wants to address.

4. b. Do we have a problem to solve?
    - "Do we have a problem to solve?" is one of the three types of questions in data analytics.

5. c. Direct Revenue Loss
    - Direct Revenue Loss, such as fewer sales, is a common reason for solving a problem, as it directly impacts profitability.

6. b. The process of examining, transforming and interpreting data to discover meaningful patterns and trends
    - Data Analytics is defined as the process of examining, transforming and interpreting data to discover meaningful patterns and trends.

7. d. Descriptive Analytics
    - Descriptive Analytics answers "What happened".

8. b. Diagnostic Analytics
    - Diagnostic Analytics explores "Why that happened" by identifying causes and patterns.

9. c. Predictive Analytics
    - Predictive Analytics uses historical data to make predictions about future outcomes.

10. d. Prescriptive Analytics
    - Prescriptive Analytics recommends actions to achieve a desired outcome.

11. c. Gathering relevant data from different sources
    - Data collection is defined as gathering relevant data from different sources, such as databases, sensors, social media, or surveys.

12. b. Surveys
    - Surveys are listed as a source for data collection.

13. b. To create a comprehensive dataset to fulfill an objective
    - The goal of data collection is to get a comprehensive dataset to fulfill an objective or address a problem.

14. c. Preparing the data by removing errors, duplicates, and inconsistencies
    - Data Cleaning is defined as preparing the data by removing errors, duplicates, and inconsistencies, ensuring it's accurate and ready for analysis.

15. b. Handle missing values
    - "Handle missing values" is a keyword associated with data cleaning.

16. a. To get a dataset ready for analysis with minimal biases or errors
    - The goal of data cleaning is to get a dataset ready for analysis with minimal biases or errors and consistently formatted.

17. c. Handling Missing Values
    - Handling Missing Values is the first step in the summary of data cleaning steps.

18. b. Remove the rows or adjust analysis to account for returned items
    - If negative quantities signify returned items, the analysis should be adjusted, or the rows should be removed.

19. b. Marked as (missing)
    - In the example, "Invalid Date" in the Purchase Date field was marked as (missing).

20. c. Set to 0%
    - In the example, blank "Discount" fields were set to 0%.

21. b. The first step of analyzing data
    - Data Exploration is often referred to as the first step of analyzing data.

22. c. While data exploration, more cleaning might be needed.
    - Data exploration and data cleaning is often an iterative process, meaning while data exploration, more cleaning might be needed.

23. b. Check for missing/null values and decide how to handle them
    - Checking for missing/null values is part of "Quality Assessment," an unofficial step in data exploration.

24. c. Descriptive Statistics (Univariate Analysis)
    - Calculating the average, minimum, and maximum sales for each month is an example of Descriptive Statistics (Univariate Analysis).

25. c. Bar Chart of Sales by Product Category
    - A Bar Chart of Sales by Product Category shows which categories generate the most revenue.

26. b. Histogram of Order Values
    - A Histogram of Order Values displays the distribution of individual order totals.

27. d. Scatter Plot of Sales vs. Advertising Spend
    - A Scatter Plot of Sales vs. Advertising Spend assesses correlation between total sales and ad spend.

28. c. The process of creating the “blueprint” for how you’ll store and organize your data
    - Data Modeling is defined as the process of creating the "blueprint" for how you’ll store and organize your data.

29. b. Specific business needs
    - Data models are built around the organization’s specific business needs.

30. c. Entities
    - "Entities" are listed as one of the key parts of data modeling.

31. c. Gather Business Requirements
    - "Gather Business Requirements" is the first step in the provided steps of Data Modeling, though noted as "Not in PDF" in that section.

32. c. Conceptual Model
    - The Conceptual Model involves sketching high-level entity-relationship diagrams showing main entities and relationships without detailed attributes.

33. d. Detail Attributes & Keys (Logical Model)
    - The Logical Model adds more detail, defining all entities, attributes, and relationships, and includes data types and constraints without specifying physical implementation.

34. c. The stage where you turn raw analysis results into clear, concise insights
    - Data interpretation is the stage where you take the raw results of your analysis and turn them into clear, concise insights.

35. c. Summarize Insights
    - Summarizing insights is a key element of interpretation under "Communicate Findings".

36. b. To call out data or analysis caveats
    - Identifying limitations involves calling out data or analysis caveats to explain how they might affect confidence in conclusions.

37. b. Data Preparation
    - The "2 Steps Process" alternative take on data analytics begins with Data Preparation.

38. c. The process of using statistical summaries and visualizations to systematically uncover patterns, anomalies, and relationships in a dataset.
    - Exploratory Data Analysis (EDA) is defined as the process of using statistical summaries and visualizations to systematically uncover the structure, patterns, anomalies, and relationships in a dataset.

39. c. Define the Problem & Understand Your Data
    - The first step in any data analysis project, including EDA, is to fully understand the problem and the data.

40. b. Verify Data Types
    - Verifying data types is an action performed during the initial inspection of data in "Step 2: Import & Inspect the Data".

41. b. Imputation of missing data
    - Imputation preserves dataset size but must be done thoughtfully when handling missing data.

42. c. Mean or median imputation
    - Simple imputation methods suggested for numerical fields include mean or median imputation.

43. b. Checking Distributions & Spot Oddities
    - Checking distributions and spotting oddities is part of "Step 4: Exploring Your Data’s Key Characteristics".

44. c. Rescales numeric features into a fixed range (usually 0 to 1).
    - Min-Max Scaling rescales numeric features into a fixed range (usually 0 to 1).

45. c. One-Hot Encoding
    - One-Hot Encoding transforms a categorical variable with *k* distinct values into *k* binary (0/1) columns.

46. c. Log Transformation
    - Logarithms are useful for reducing right-skew and stabilizing variance.

47. c. Bar charts or pie charts
    - For categorical variables, bar charts or pie charts are used to compare category counts, spot imbalances, and highlight unexpected groupings.

48. c. Histograms, box plots, violin plots or density curves
    - These visualizations are used for numerical variables to examine each feature’s distribution, central tendency, variability, and outliers.

49. c. Communicate Your Findings
    - The final step listed for doing EDA is "Step 7: Communicate Your Findings".

50. b. Use charts and tables to illustrate key results
    - Leveraging visuals like charts and tables helps illustrate key results and make complex insights more digestible when communicating findings.